# âš¡ 4 Linear Algebra + Calculus Together â†’ Gradient Descent

---

## ðŸš€ Why Gradient Descent?

* Core of **machine learning training** â†’ helps models **learn**.
* Itâ€™s about finding the **minimum error** of a cost (loss) function.
* Think of it as **searching for the lowest valley point** in a curve.

ðŸ”¹ **EEE Analogy:**
Just like **tuning PID controller gains** to minimize steady-state error, in ML we tune **model parameters (weights & biases)** to minimize error.

---

## ðŸ§® Key Concepts Linking Linear Algebra + Calculus

1. **Linear Algebra (Vectors & Matrices)**

   * Parameters (weights) are stored as **vectors/matrices**.
   * Input data also comes as **matrices**.
   * Example: For linear regression

     $$
     y = Xw + b
     $$

     where

     * $X$: Input matrix (features)
     * $w$: Weight vector
     * $b$: Bias

2. **Calculus (Derivatives & Slopes)**

   * To minimize error, we need to know the **direction of steepest descent**.
   * The derivative (gradient) gives the **slope**.
   * If slope is positive â†’ go left,
     If slope is negative â†’ go right.

3. **Gradient**

   * Multivariable derivative (vector of partial derivatives).
   * For cost function $J(w)$:

     $$
     \nabla J(w) = \left[ \frac{\partial J}{\partial w_1}, \frac{\partial J}{\partial w_2}, \dots \right]
     $$

4. **Gradient Descent Update Rule**

   * Iterative process:

     $$
     w := w - \eta \cdot \nabla J(w)
     $$

     * $w$: parameters (weights)
     * $\eta$: learning rate (step size)
     * $\nabla J(w)$: gradient (direction to move)

---

## ðŸ“‰ Example: Linear Regression with Gradient Descent

Suppose we want to fit a line:

$$
y = wx + b
$$

* **Cost function (MSE):**

$$
J(w, b) = \frac{1}{n}\sum_{i=1}^{n}(y_i - (wx_i + b))^2
$$

* **Compute Gradient:**

$$
\frac{\partial J}{\partial w} = -\frac{2}{n}\sum (x_i)(y_i - (wx_i + b))
$$

$$
\frac{\partial J}{\partial b} = -\frac{2}{n}\sum (y_i - (wx_i + b))
$$

* **Update Rule:**

$$
w := w - \eta \cdot \frac{\partial J}{\partial w}
$$

$$
b := b - \eta \cdot \frac{\partial J}{\partial b}
$$

---

## ðŸ”§ Numerical Example

Suppose dataset (2 points):

$$
(x, y) = \{(1, 2), (2, 3)\}
$$

1. Start with initial guess $w=0, b=0$.
2. Compute gradients:

   * For $w$: $-\frac{2}{2}[(1)(2-0) + (2)(3-0)] = -16/2 = -8$
   * For $b$: $-\frac{2}{2}[(2-0) + (3-0)] = -10/2 = -5$
3. Update with learning rate $\eta=0.1$:

   $$
   w := 0 - 0.1(-8) = 0.8
   $$

   $$
   b := 0 - 0.1(-5) = 0.5
   $$

âœ… New line â†’ $y = 0.8x + 0.5$ (closer to actual data).

---

## ðŸŽ¯ Visual Analogy

* Imagine a ball rolling down a hill.
* Each step, it moves in the **direction of steepest slope** (negative gradient).
* Learning rate = size of each step.
* Too small â†’ slow learning.
* Too large â†’ overshooting the valley.

---

## âš¡ Takeaway

* Gradient descent is the **heart of optimization in ML**.
* It combines **linear algebra (matrix operations)** + **calculus (gradients)**.
* Used in:

  * Linear/Logistic Regression
  * Neural Networks
  * Deep Learning

---
