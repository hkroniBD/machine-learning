# üìò Lecture 5: Statistics for Model Evaluation

---

## üåü Why?

üìå After training a model, we must answer: *Is it performing well or just guessing?*
Statistical measures help us **quantify** performance.

---

## üîë Key Topics

### 1Ô∏è‚É£ Mean Squared Error (MSE)

* Used in **regression models** (predicting continuous values like load demand, temperature).
* Formula:

$$
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

* Example: Predicting power consumption. If actual = \[100, 120, 130], predicted = \[110, 115, 128] ‚Üí MSE ‚âà 29.

---

### 2Ô∏è‚É£ Accuracy

* Simple but limited.

$$
Accuracy = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
$$

* Example: Fault detection system predicted 90 faults correctly out of 100 ‚Üí Accuracy = 90%.
* ‚ö†Ô∏è Problem: If 95% of data is "No Fault", a model that always says "No Fault" gives 95% accuracy but is useless!

---

### 3Ô∏è‚É£ Precision

* Out of all predicted positives, how many are correct?

$$
Precision = \frac{TP}{TP + FP}
$$

* Example: In fault detection, the model predicted 20 faults, but only 15 were real ‚Üí Precision = 15/20 = 75%.

---

### 4Ô∏è‚É£ Recall (Sensitivity)

* Out of all actual positives, how many did we catch?

$$
Recall = \frac{TP}{TP + FN}
$$

* Example: There were 25 real faults, but the model caught only 15 ‚Üí Recall = 15/25 = 60%.

---

### 5Ô∏è‚É£ F1 Score

* Balances **Precision** and **Recall** using harmonic mean.

$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

* Example: With Precision = 0.75, Recall = 0.60 ‚Üí

$$
F1 = 2 \times \frac{0.75 \times 0.60}{0.75 + 0.60} = 0.67
$$

* ‚úÖ Useful in **imbalanced datasets** (e.g., rare events like short-circuit faults).

---

### 6Ô∏è‚É£ Confusion Matrix

A table showing performance at a glance:

|                     | Predicted Positive  | Predicted Negative  |
| ------------------- | ------------------- | ------------------- |
| **Actual Positive** | True Positive (TP)  | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN)  |

* Example (Fault detection):

  * TP = 15 (correctly detected faults)
  * FN = 10 (missed faults)
  * FP = 5 (false alarms)
  * TN = 70 (correctly identified normal)

---

## ‚ö° Real-World EEE Example

* Suppose you build a model to detect transformer overheating.

  * High **Recall** ‚Üí catches all dangerous overheating cases (safety first).
  * High **Precision** ‚Üí avoids false alarms that waste maintenance costs.
  * **F1 Score** ‚Üí balances the two, ensuring both safety and efficiency.

---

## üéØ Summary

* **MSE** ‚Üí For regression tasks.
* **Accuracy** ‚Üí Overall correctness (but tricky in imbalance).
* **Precision & Recall** ‚Üí Quality of classification.
* **F1 Score** ‚Üí Balanced measure.
* **Confusion Matrix** ‚Üí Detailed breakdown of model performance.

---
