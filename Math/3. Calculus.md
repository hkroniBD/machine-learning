# **Lecture 3: Calculus for Machine Learning**

*(Essential Differentiation & Integration Concepts)*

---

## ðŸ”¹ **Why Calculus in ML?**

* Optimization is at the heart of ML â†’ requires **derivatives**.
* Neural networks â†’ weights updated using **gradients**.
* Signal & system modeling â†’ differential equations form the backbone.

ðŸ“Œ **EEE Connection:**

* Loss minimization in training ML models â‰ˆ minimizing power loss in circuits.
* Backpropagation in neural networks = repeated application of **chain rule**.

---

## ðŸ§® **1. Differentiation Basics**

* **Derivative = rate of change**

  * $f'(x) = \frac{df}{dx}$

ðŸ”¸ Examples:

* $f(x) = x^2$ â†’ $f'(x) = 2x$
* $f(x) = e^x$ â†’ $f'(x) = e^x$

ðŸ“Œ **ML Link:**

* Slope indicates **how fast loss changes** when weights are updated.

ðŸ’¡ **EEE Example:**

* Output voltage $V_{out}(t) = t^2$.
* Rate of change: $\frac{dV}{dt} = 2t$.
* Same as finding how fast **loss decreases** in ML training.

---

## ðŸ”¹ **2. Gradient (Multi-variable Derivatives)**

* For $f(x,y)$:

  $$
  \nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)
  $$

ðŸ“Œ **ML Use:**

* Gradient tells the **steepest descent** â†’ used in **Gradient Descent Algorithm**.

ðŸ’¡ **EEE Example:**

* Power loss: $P(R,I) = I^2R$.
* Gradient:

  $$
  \frac{\partial P}{\partial I} = 2IR, \quad \frac{\partial P}{\partial R} = I^2
  $$
* Similar to updating **two weights** in ML simultaneously.

---

## ðŸ”¹ **3. Chain Rule**

* If $y = f(g(x))$:

  $$
  \frac{dy}{dx} = f'(g(x)) \cdot g'(x)
  $$

ðŸ“Œ **ML Use:**

* Backpropagation in neural networks uses **chain rule repeatedly**.

ðŸ’¡ **EEE Example:**

* Current through capacitor: $I = C \frac{dV}{dt}$.
* If $V(t) = \sin(\omega t)$:

  $$
  I = C \cdot \omega \cos(\omega t)
  $$
* Exactly how signals propagate in **deep networks**.

---

## ðŸ”¹ **4. Integration (Basics)**

* Reverse of differentiation.
* $$
  \int f(x) dx
  $$

ðŸ“Œ **ML Use:**

* Probabilities (area under curve).
* Normalization of probability density functions (PDFs).

ðŸ’¡ **EEE Example:**

* Energy stored in capacitor:

  $$
  E = \int V \, dQ = \frac{1}{2}CV^2
  $$
* Same way ML computes **expected values** over distributions.

---

## ðŸ”¹ **5. Gradient Descent in ML**

* Update rule:

  $$
  w_{new} = w_{old} - \eta \cdot \frac{dL}{dw}
  $$

  where $\eta$ = learning rate, $L$ = loss

ðŸ’¡ **EEE Example (Analogy):**

* Imagine adjusting a transformer tap to **minimize voltage error** â†’ each adjustment = a step in **gradient descent**.

---

## ðŸŽ¯ **Takeaways**

* Differentiation â†’ sensitivity of loss.
* Gradients â†’ direction of fastest learning.
* Chain Rule â†’ backbone of backpropagation.
* Integration â†’ probabilities in ML models.

---

## âœ… **Mini-Exercise for Students**

1. Find derivative of $f(x) = 3x^2 + 2x + 1$.
2. Compute gradient of $f(x,y) = x^2 + y^2$.
3. If loss $L(w) = (w-5)^2$, update weight $w=2$ with $\eta = 0.1$.

---
